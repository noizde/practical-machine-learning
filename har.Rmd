---
title: "Human Activity Recognition"
author: "Albert Dizon"
date: "February 27, 2016"
output: html_document
---

This activity uses data from movement-measuring devices, taken from users asked to perform the Unilateral Dumbbell Biceps Curl in five different manners (encoded as "A" through "E"). Using this data, we aim to be able to correctly predict/classify (using new data) the manner in which future such movements are done.  

We begin by loading the datasets and the Caret model training package.

```{r loadData}
library(caret);
set.seed(772349);

finalTest <- read.csv("data/pml-testing.csv");
initTrain <- read.csv("data/pml-training.csv");
```

An exploratory look at the data shows that some cleaning is necessary before we can use them for training: some columns are irrelevant and may add unnecessary computations (e.g. user name and timestamp), while other columns have been encoded as discrete factor variables, rather than continuous numeric variables, which can distort the way the relationships between these variables are interpreted.

The data also contain many blank values, and how to interpret them is not immediately obvious. Given the normal range of values, and that the distribution of NAs across the various measures did not seem to explicitly suggest any one classification or another, I chose to interpret NAs as denoting a negligible amount of movement for that measure, I chose to recode them to be zeroes.

``` {r cleanData, cache = TRUE, warning = FALSE}
cleanData <- function(data) {
  # Remove X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window
  df <- data[,-c(1:7)];
  # Turn factor variables into numerics (apart from classe), turn NAs into 0s.
  df <- cbind(apply(df[1:152], c(1,2), function(e){
    n <- as.numeric(as.character(e));
    if (is.na(n)) {
      0
    } else {
      n
    }
  }), df[153]);
}

cleanTrain <- cleanData(initTrain);
```

Next, we eliminate the variables which don't significantly vary between observations (thus having limited explanatory value), which, on the other hand, significantly pares down the number of predictors to take into account for building our models. The "nzv" column (used later) holds a boolean vector of which columns we can exclude.

[(Note: this does not apply in cases when the small variance *is* closely correlated with the statistic of interest, which does not seem to be so in this case)](http://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/)

``` {r variance, cache = TRUE}
nearZeroVarCols <- nearZeroVar(cleanTrain, saveMetrics = TRUE);
```

Now that the data are in a more consistent state, we can partition the set into training and testing sets.

``` {r dividing, cache = TRUE}
inTrain <- createDataPartition(cleanTrain$classe, p = 2/3)[[1]];
training <- cleanTrain[inTrain, !nearZeroVarCols$nzv];
testing <- cleanTrain[-inTrain, !nearZeroVarCols$nzv];
```

We try four different models for the data: an rpart classification tree, a generalized boosted regression model combining multiple predictors, a linear discriminant analysis model looking at linear relationships between the predictors, and a random forest model using many bootstrapped trees to determine classification. Within each model, we specify it to use 3-fold cross validation of the supplied training set.

```{r fitting, cache = TRUE, message = FALSE}
fit1 <- train(classe ~ ., data = training, method = "rpart", trControl = trainControl(method = "cv", number = 3));
fit2 <- train(classe ~ ., data = training, method = "gbm", trControl = trainControl(method = "cv", number = 3), verbose = FALSE); # gbm tends to print a lot of messages
fit3 <- train(classe ~ ., data = training, method = "lda", trControl = trainControl(method = "cv", number = 3));
fit4 <- train(classe ~ ., data = training, method = "rf", trControl = trainControl(method = "cv", number = 3));
```

We then make predictions with these models using data from the testing set, and construct confusion matrices for each, to see how accurate each model is for totally new data.

``` {r testing, cache = FALSE, message = FALSE}
prconf <- function(fit, data) {
  pred <- predict(fit, newdata = data);
  confusionMatrix(pred, data$classe);
}

prconf(fit1, testing); # rpart
prconf(fit2, testing); # gbm
prconf(fit3, testing); # lda
prconf(fit4, testing); # rf

```

According to the predictions, the rf and gbm models have accuracies 99% and 96%, respectively, which means they should be good classifiers for the final testing set that we will make predictions on. To get a feel for how well the four models agree with each other, we lay them out in a data frame, arranged by accuracy (least to greatest).

``` {r finalTesting}

cleanTest <- cleanData(finalTest);

t1 <- predict(fit1, finalTest); # rpart: 56% accuracy
t2 <- predict(fit2, finalTest); #   gbm: 96% accuracy
t3 <- predict(fit3, finalTest); #   lda: 70% accuracy
t4 <- predict(fit4, finalTest); #    rf: 99% accuracy

finalPredictions <- data.frame(rpart = t1, lda = t3, gbm = t2, rf = t4);
```

We then have our predictions for the actual testing set of the activity:

``` {r finalPred}
finalPredictions
```